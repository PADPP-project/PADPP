tokenizer: facebook/bart-large
plm: facebook/bart-large
lm_size: 1024
per_device_train_batch_size: 32
per_device_eval_batch_size: 32
combined_action: True
max_sequence_length: 512
cached_dir: "cache/BART/"
weight_decay: 0.01
learning_rate: 0.0001
num_warmup_steps: 3000
gradient_accumulation_steps: 1
num_train_epochs: 5
dropout: 0.1
max_grad_norm: 5
max_gen_length: 32
max_target_length: 50