tokenizer: roberta-large
plm: roberta-large
lm_size: 1024
per_device_train_batch_size: 16
per_device_eval_batch_size: 16
max_sequence_length: 512
cached_dir: "cache/MODPL/"
weight_decay: 0.01
learning_rate: 0.00005
num_warmup_steps: 3000
gradient_accumulation_steps: 1
num_train_epochs: 5
dropout: 0.1
max_grad_norm: 5
run_sft: true
run_rlt: true
run_online_eval: true
run_offline_eval: true
num_train_rl_epochs: 10
train_rl_batch_size: 128
sampled_times: 50
freeze_preference_backbone: true
num_train_preference_epochs: 10
preference_warmup_steps: 1000
reward_hidden_size: 16
preference_batch_size: 8
num_train_ppo_epochs: 10
preference_buffer_length: 50
ppo_buffer_length: 2000
actor_warmup_steps: 1000
critic_warmup_steps: 1000