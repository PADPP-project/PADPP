tokenizer: facebook/bart-base
plm: facebook/bart-base
lm_size: 768
per_device_train_batch_size: 16
per_device_eval_batch_size: 16
max_sequence_length: 512
max_target_length: 100
max_gen_length: 50
cached_dir: "cache/UNIMIND/"
weight_decay: 0.01
learning_rate: 0.00005
num_warmup_steps: 3150
gradient_accumulation_steps: 1
max_transition_number: 11
latent_dim: 16
use_simulated: true
freeze_plm: true
eval_brownian_bridge: true
bridge_learning_rate: 0.0002
train_use_bridge: true
use_transform: false
use_KLD: false
trans_alpha: 0.1
gen_beta: 1.0
kl_gamma: 1.0
planner_learning_rate: 0.00002
num_bridge_epochs: 10
num_planner_epochs: 5
dropout: 0.1
max_grad_norm: 5
run_sft: false
run_online_eval: true
run_offline_eval: false