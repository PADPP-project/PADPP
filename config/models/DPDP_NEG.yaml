tokenizer: roberta-large
plm: roberta-large
lm_size: 1024
per_device_train_batch_size: 8
per_device_eval_batch_size: 8
max_sequence_length: 512
cached_dir: "cache/DPDP/"
weight_decay: 0.01
learning_rate: 0.000005
num_warmup_steps: 400
gradient_accumulation_steps: 1
num_train_epochs: 2
dropout: 0.1
max_grad_norm: 5
run_sft: false
run_rlt: false
run_online_eval: true
run_offline_eval: false
num_train_rl_epochs: 50
sampled_times: 10
rl_learning_rate: 0.000006
uniform_weights: false
