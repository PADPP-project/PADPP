tokenizer: bert-base-cased
plm: bert-base-cased
lm_size: 768
hidden_size: 128
per_device_train_batch_size: 8
per_device_eval_batch_size: 8
combined_action: True
max_sequence_length: 512
cached_dir: "cache/BERT/"
weight_decay: 0.01
learning_rate: 0.0001
num_warmup_steps: 3000
gradient_accumulation_steps: 1
num_train_epochs: 5
dropout: 0.1
max_grad_norm: 5
run_sft: True
